{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - preprocesamiento_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmSf70qa41Ut"
      },
      "source": [
        "<a href=\"https://www.inove.com.ar\"><img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/PA%20Banner.png\" width=\"1000\" align=\"center\"></a>\n",
        "\n",
        "\n",
        "# NLP - Proposecamiento\n",
        "Ejemplo de como aplicar preprocesamiento de texto.\n",
        "\n",
        "v1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import random \n",
        "import numpy as np\n",
        "import re\n",
        "import pickle"
      ],
      "metadata": {
        "id": "MXwyiPUBdA3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Herramientas de preprocesamiento de datos\n",
        "Entre las tareas de procesamiento de texto en español se implementa:\n",
        "- Quitar números\n",
        "- Quitar símbolos de puntuación\n",
        "- Quitar caracteres acentuados"
      ],
      "metadata": {
        "id": "d7kWJClU0Zs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation + \"¡\" + \"¿\""
      ],
      "metadata": {
        "id": "rZsr8Em107nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_clean_text(text):\n",
        "    # pasar a minúsculas\n",
        "    text = text.lower()\n",
        "    # quitar números\n",
        "    pattern = r'[0-9\\n]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    # quitar caracteres de puntiación\n",
        "    text = ''.join([c for c in text if c not in (string.punctuation+\"¡\"+\"¿\")])\n",
        "    # quitar caracteres con acento\n",
        "    text = re.sub(r'[àáâä]', \"a\", text)\n",
        "    text = re.sub(r'[éèêë]', \"e\", text)\n",
        "    text = re.sub(r'[íìîï]', \"i\", text)\n",
        "    text = re.sub(r'[òóôö]', \"o\", text)\n",
        "    text = re.sub(r'[úùûü]', \"u\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "NEPO44To0XDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos ver como los símbolos y números son removidos del texto\n",
        "preprocess_clean_text(\"¿como5!\")"
      ],
      "metadata": {
        "id": "iJV1-EYR1BYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos ver como los tíldes y mayúsculas son removidos del texto\n",
        "preprocess_clean_text(\"Cómo está el DÍA de hOY\")"
      ],
      "metadata": {
        "id": "qxTFKFNjz87-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizador\n",
        "El proceso de tokenizar es dividir el texto en elementos individuales (palabras) los cuales llamaremos \"tokens\""
      ],
      "metadata": {
        "id": "b-lpcTq50kWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crearemos el tokenizador más simple de todos con la función \"split\""
      ],
      "metadata": {
        "id": "i938UASH1EQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_clean_text(\"Cómo está el DÍA de hOY\").split(\" \")"
      ],
      "metadata": {
        "id": "AwESEUi91RRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OneHotEncoding\n",
        "Para poder armar el OneHotEncoding es necesario contar con todas las palabras que conformar al vocabulario del dataset"
      ],
      "metadata": {
        "id": "5_K0J7zp1WyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"Que bueno que esta\", \"Esta muy bueno\", \"Esta excelente\"]\n",
        "words = []\n",
        "for doc in corpus:\n",
        "    tokens = preprocess_clean_text(doc).split(\" \")\n",
        "    for token in tokens:\n",
        "        words.append(token)\n",
        "\n",
        "words"
      ],
      "metadata": {
        "id": "VpxGEhhf1xHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El vaculario son todas las palabras del courpus sin repetirse. Utilizaremos \"set\" para quitar las palabras repetidas"
      ],
      "metadata": {
        "id": "UQfK4Jtm2joM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(words))\n",
        "vocab"
      ],
      "metadata": {
        "id": "kiLrlS3J2qLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El tamaño del vector de OneHotEncoding es del tamaño del vocabulario. Los vectores los llaremos \"bag of words\" (bow)"
      ],
      "metadata": {
        "id": "AgFT43wL2y8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)\n",
        "for doc in corpus:\n",
        "    bow = []\n",
        "    doc_procesado = preprocess_clean_text(doc)\n",
        "    for word in vocab:\n",
        "        bow.append(1) if word in doc_procesado else bow.append(0)\n",
        "    print(doc_procesado, bow)"
      ],
      "metadata": {
        "id": "ldYJ3X_w27CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline7.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "WAjCJvYk5o17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya contamos con el texto transformado en números, ¿pero será suficiente?\\\n",
        "La técnica utilizada de OneHotEncoding en NLP es muy fácil de implementar peor a su vez muy simple, tiene un problema muy importante --> El tamaño de los vectores BOW crecerán a medida que crezca el vocabulario."
      ],
      "metadata": {
        "id": "NxpV7eXr5p5E"
      }
    }
  ]
}