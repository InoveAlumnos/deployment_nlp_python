{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmSf70qa41Ut"
      },
      "source": [
        "<a href=\"https://www.inove.com.ar\"><img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/PA%20Banner.png\" width=\"1000\" align=\"center\"></a>\n",
        "\n",
        "\n",
        "# NLP - Bot basado en reglas con Tensorflow\n",
        "Este ejemplo consiste en armar BOT simple basado en una red neuronal con Tensorflow\n",
        "\n",
        "v1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import random \n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "MXwyiPUBdA3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recolectar datos\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline1.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "AZJ-9ME-zz9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset en formato JSON que representa las posibles preguntas (patterns)\n",
        "# y las posibles respuestas por categoría (tag)\n",
        "data = {\"intents\": [\n",
        "             {\"tag\": \"bienvenida\",\n",
        "              \"patterns\": [\"Hola\", \"¿Cómo estás?\", \"¿Qué tal?\"],\n",
        "              \"responses\": [\"Hola!\", \"Hola, ¿Cómo estás?\"],\n",
        "             },\n",
        "             {\"tag\": \"nombre\",\n",
        "              \"patterns\": [\"¿Cúal es tu nombre?\", \"¿Quién sos?\"],\n",
        "              \"responses\": [\"Mi nombre es MarvelBOT\", \"Yo soy MarvelBOT\"]\n",
        "             },\n",
        "            {\"tag\": \"contacto\",\n",
        "              \"patterns\": [\"contacto\", \"número de contacto\", \"número de teléfono\", \"número de whatsapp\", \"whatsapp\"],\n",
        "              \"responses\": [\"Podes contactarnos al siguiente número +54-9-11-2154-4777\", \"Contactonos al whatsapp número +54-9-11-2154-4777\"]\n",
        "             },\n",
        "            {\"tag\": \"envios\",\n",
        "              \"patterns\": [\"¿Realizan envios?\", \"¿Cómo me llega el paquete?\"],\n",
        "              \"responses\": [\"Los envios se realizan por correo, lo enviaremos a la dirección que registraste en la página\"]\n",
        "             },\n",
        "            {\"tag\": \"precios\",\n",
        "              \"patterns\": [\"precio\", \"Me podrás pasar los precios\", \"¿Cuánto vale?\", \"¿Cuánto sale?\"],\n",
        "              \"responses\": [\"En el catálogo podrás encontrar los precios de todos nuestros productos en stock\"]\n",
        "             },\n",
        "            {\"tag\": \"pagos\",\n",
        "              \"patterns\": [\"medios de pago\", \"tarjeta de crédito\", \"tarjetas\", \"cuotas\"],\n",
        "              \"responses\": [\"Contactanos al whatsapp número +54-9-11-2154-4777 para conocer los beneficios y formas de pago vigentes\"]\n",
        "             },\n",
        "            {\"tag\": \"stock\",\n",
        "              \"patterns\": [\"Esto está disponible\", \"¿Tenes stock?\", \"¿Hay stock?\"],\n",
        "              \"responses\": [\"Los productos publicados están en stock\"]\n",
        "             },\n",
        "            {\"tag\": \"agradecimientos\",\n",
        "              \"patterns\": [ \"Muchas gracias\", \"Gracias\"],\n",
        "              \"responses\": [\"Por nada!, cualquier otra consulta podes escribirnos\"]\n",
        "             },\n",
        "             {\"tag\": \"despedida\",\n",
        "              \"patterns\": [ \"Chau\", \"Hasta luego!\"],\n",
        "              \"responses\": [\"Hasta luego!\", \"Hablamos luego!\"]\n",
        "             }\n",
        "]}"
      ],
      "metadata": {
        "id": "1D8L5lA4zmMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesar datos\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline2.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "PM2E2ylVz9HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Herramientas de preprocesamiento de datos\n",
        "Entre las tareas de procesamiento de texto en español se implementa:\n",
        "- Quitar números\n",
        "- Quitar símbolos de puntuación\n",
        "- Quitar caracteres acentuados"
      ],
      "metadata": {
        "id": "d7kWJClU0Zs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# El preprocesamento en castellano requiere más trabajo\n",
        "\n",
        "def preprocess_clean_text(text):\n",
        "    # pasar a minúsculas\n",
        "    text = text.lower()\n",
        "    # quitar números\n",
        "    pattern = r'[0-9\\n]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    # quitar caracteres de puntiación\n",
        "    text = ''.join([c for c in text if c not in (string.punctuation+\"¡\"+\"¿\")])\n",
        "    # quitar caracteres con acento\n",
        "    text = re.sub(r'[àáâä]', \"a\", text)\n",
        "    text = re.sub(r'[éèêë]', \"e\", text)\n",
        "    text = re.sub(r'[íìîï]', \"i\", text)\n",
        "    text = re.sub(r'[òóôö]', \"o\", text)\n",
        "    text = re.sub(r'[úùûü]', \"u\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "NEPO44To0XDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation + \"¡\" + \"¿\""
      ],
      "metadata": {
        "id": "rZsr8Em107nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_clean_text(\"¿cómo5!\")"
      ],
      "metadata": {
        "id": "iJV1-EYR1BYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematizacion"
      ],
      "metadata": {
        "id": "vzE4ZjN-6lln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('lematizacion-es.pickle', os.F_OK) is False:\n",
        "    !curl -L -o 'lematizacion-es.zip' 'https://drive.google.com/u/0/uc?id=16leuM9PuFXAkmw34XeQy-84h8WGAYxJw&export=download&confirm=t'\n",
        "    !unzip -q lematizacion-es.zip\n",
        "else:\n",
        "    print(\"El archivo ya se encuentra descargado\")"
      ],
      "metadata": {
        "id": "QvETOouHynAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lematizacion-es.pickle\",'rb') as fi:\n",
        "    lemma_lookupTable = pickle.load(fi)"
      ],
      "metadata": {
        "id": "o1BGCceizG5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del funcionamiento de lematización\n",
        "palabras_ensayo = [\"estar\", \"estoy\", \"estás\", \"está\", \"estamos\" ,\"estais\", \"estan\", \"estaremos\", \"estuvieron\"]\n",
        "for palabra in palabras_ensayo:\n",
        "    print(f\"{palabra} -> {lemma_lookupTable.get(palabra)}\")"
      ],
      "metadata": {
        "id": "dKCBpR2Z_u6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesado del dataset"
      ],
      "metadata": {
        "id": "h2Z_jqvQ_xm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "doc_X = []\n",
        "doc_y = []\n",
        "# Tokenizar cada \"pattern\" y agregar cada palabra al vocabulario (vocabulary)\n",
        "# Los tokens que se toman de cada pattern se agrega a doc_X\n",
        "# Cada tag se agrega a doc_y\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        # trasformar el patron a tokens\n",
        "        tokens = preprocess_clean_text(pattern).split(\" \")\n",
        "        # lematizar los tokens\n",
        "        lemma_words = []\n",
        "        for token in tokens:\n",
        "            lemma = lemma_lookupTable.get(token)\n",
        "            if lemma is not None:\n",
        "                lemma_words.append(lemma)\n",
        "            else:\n",
        "                print(\"UNK:\", token)\n",
        "        \n",
        "        if not lemma_words:\n",
        "            continue\n",
        "        \n",
        "        words += lemma_words\n",
        "        doc_X.append(pattern)\n",
        "        doc_y.append(intent[\"tag\"])\n",
        "    \n",
        "    # Agregar el tag a las clases\n",
        "    if intent[\"tag\"] not in classes:\n",
        "        classes.append(intent[\"tag\"])\n",
        "\n",
        "# Elminar duplicados con \"set\" y ordenar el vocubulario y las clases por orden alfabético\n",
        "vocab = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "X9HsCV870EDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explorar datos\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline3.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "VMQcLbEM3fTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocab:\", vocab)\n",
        "print(\"classes:\", classes)\n",
        "print(\"doc_X:\", doc_X)\n",
        "print(\"doc_y:\", doc_y)"
      ],
      "metadata": {
        "id": "tUweP3bG3hUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_y_encoded = [classes.index(label) for label in doc_y]\n",
        "doc_y_encoded"
      ],
      "metadata": {
        "id": "Po1JduvCpezn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar modelo\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline4.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "_PUzAtcW3zsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "for doc in doc_X:\n",
        "    # Transformar la pregunta (input) en tokens y lematizar\n",
        "    lemma_words = []\n",
        "    tokens = preprocess_clean_text(doc).split(\" \")\n",
        "    for token in tokens:\n",
        "        lemma = lemma_lookupTable.get(token)\n",
        "        if lemma is not None:\n",
        "            lemma_words.append(lemma)\n",
        "\n",
        "    # Transformar los tokens en \"Bag of words\" (arrays de 1 y 0)\n",
        "    bow = []\n",
        "    for word in vocab:\n",
        "        bow.append(1) if word in lemma_words else bow.append(0)\n",
        "    \n",
        "    print(\"X:\", bow)\n",
        "    X_train.append(bow)\n",
        "\n",
        "X_train = np.array(X_train)"
      ],
      "metadata": {
        "id": "t7s9zLeaPByu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "G_zR85n9sE3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.keras.utils.to_categorical(doc_y_encoded)\n",
        "y_train[:4]"
      ],
      "metadata": {
        "id": "A6gfIh3Vou_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X_train.shape[1]\n",
        "output_shape = y_train.shape[1]\n",
        "print(\"input:\", input_shape, \"output:\", output_shape)"
      ],
      "metadata": {
        "id": "GtyHpTUarCVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo DNN\n",
        "# - Modelo secuencial\n",
        "# - Con regularización\n",
        "# - softmax y optimizador Adam\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation=\"relu\", input_shape=(input_shape,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(output_shape, activation = \"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "AXA82FbedobS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x=X_train, y=y_train, epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "cGpqs-HGrelu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WlWHk0mxt9dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizar modelo\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline6.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "X8Mt0k2auCYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responses = [[\"\"]] * len(classes)\n",
        "for intent in data[\"intents\"]:\n",
        "    responses[classes.index(intent[\"tag\"])] = intent[\"responses\"]\n",
        "\n",
        "responses"
      ],
      "metadata": {
        "id": "JtkB3SyV55_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    message = input(\"\")\n",
        "\n",
        "    # preprocesamiento + lematizacion\n",
        "    # ------------------------------------------\n",
        "    # Transformar la pregunta (input) en tokens y lematizar\n",
        "    lemma_words = []\n",
        "    tokens = preprocess_clean_text(message).split(\" \")\n",
        "    for token in tokens:\n",
        "        lemma = lemma_lookupTable.get(token)\n",
        "        if lemma is not None:\n",
        "            lemma_words.append(lemma)\n",
        "\n",
        "    # Transformar los tokens en \"Bag of words\" (arrays de 1 y 0)\n",
        "    bow = []\n",
        "    for word in vocab:\n",
        "        bow.append(1) if word in lemma_words else bow.append(0)\n",
        "    # ------------------------------------------\n",
        "\n",
        "    probs = model.predict([bow])\n",
        "    score = probs.max()\n",
        "    if score > 0.4:  # threshold 0.4        \n",
        "        index = probs.argmax(axis=1)[0]\n",
        "        result = random.choice(responses[index])\n",
        "        print(f\"[{score:.2f}]: {result}\")\n",
        "    else:\n",
        "        print(f\"[{score:.2f}] Perdon, no comprendo la pregunta.\")"
      ],
      "metadata": {
        "id": "6WRHqo7ZwSZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargar modelo\n",
        "<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline7.png\" width=\"1000\" align=\"middle\">"
      ],
      "metadata": {
        "id": "dk1lrxqQaZBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el repositorio de clase ya se encuentran descargados estos archivos dentro de una carpeta llamada \"model\". El código a continuación es un ejemplo de como se descargaron por si usted quiere editar el bot y descargar los archivos necesarios para ejecutarlo / deployarlo."
      ],
      "metadata": {
        "id": "vbWpLr6lae5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exportar los datos importants (vocabulario, clases, el bot y el dataset utilizado)\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "pickle.dump(vocab, open('vocab.pkl','wb'))\n",
        "pickle.dump(responses, open('responses.pkl','wb'))\n",
        "model.save('bot.h5')"
      ],
      "metadata": {
        "id": "0EHGGDYJabSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprimir todos los datos necesarios\n",
        "!zip -r bot_data.zip bot.h5 vocab.pkl responses.pkl lematizacion-es.pickle"
      ],
      "metadata": {
        "id": "IEzfmyflac4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('bot_data.zip') "
      ],
      "metadata": {
        "id": "yk-2msUPaeTb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}